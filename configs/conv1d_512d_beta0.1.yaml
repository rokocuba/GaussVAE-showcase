# Conv1D VAE with 512D Latent + Beta=0.1
# Drastic beta reduction to prevent posterior collapse from run_002 & run_003:
# - run_002 (256D, β=1.0): KL collapsed to 0.06
# - run_003 (512D, β=0.5): KL collapsed to 0.07
#
# Strategy: 10x reduction in max_beta (0.5 → 0.1)
# - Weighted KL contribution at β=0.1, KL=0.5: only 0.05
# - Reconstruction loss (≈3.2) will dominate by 64x
# - Forces model to use latent space for good reconstruction
# - KL regularization becomes gentle "preference" not hard constraint
#
# Expected improvements:
# - KL loss should remain >0.2 (not collapse to <0.1)
# - Recon loss should drop below 3.0 (model uses informative latents)
# - Most of 512 latent dimensions should stay active
# - Latent space diversity maintained throughout training

experiment_name: "run_004_512d_beta0.1"

model:
  name: "conv1d_512d_beta0.1"
  latent_dim: 512  # Same as run_003
  encoder_filters: [32, 64, 128]
  decoder_filters: [64, 32, 16]
  dropout_rates: [0.1, 0.15, 0.2]

training:
  batch_size: 32
  epochs: 100
  learning_rate: 0.001
  beta_warmup_epochs: 50
  max_beta: 0.1  # ← KEY CHANGE: 10x reduction (0.5 → 0.1)
  mixed_precision: true

data:
  train_dir: "data/delaunay/npz/train"
  dev_dir: "data/delaunay/npz/dev"
  test_dir: "data/delaunay/npz/test"
  stats_path: "data/normalization_stats.npz"
  shuffle_buffer: 1000

callbacks:
  checkpoint_every: 10
  early_stopping_patience: 15
  reduce_lr_patience: 5
  reduce_lr_factor: 0.5
  min_lr: 0.000001

output_dir: "experiments/run_004_512d_beta0.1"
seed: 42
